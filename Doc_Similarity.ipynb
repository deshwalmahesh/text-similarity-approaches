{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Requirement already satisfied: python-Levenshtein in /home/admin1/anaconda3/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in /home/admin1/anaconda3/lib/python3.8/site-packages (from python-Levenshtein) (56.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import spatial\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Manual / Statistical Thresholding</font>\n",
    "Set a threshold on any distance or metric and if two docs more/less than that metric, those are similar. Threshold depends on the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"Music is a universal language\"\n",
    "d2  =  \"Music is a miracle\"\n",
    "d3  =  \"Music is a universal feature of the human experience\"\n",
    "d = [d1,d2,d3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing: Near Duplicate Detection\n",
    "1. Build `Min / Sim ` Hashes of documents on char / word level \n",
    "2. Find the Hamming Distance between any 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Fuzzy Logic](https://hub.packtpub.com/use-tensorflow-and-nlp-to-detect-duplicate-quora-questions-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "46\n",
      "--------------------------------------------------\n",
      "72\n",
      "72\n",
      "--------------------------------------------------\n",
      "86\n",
      "86\n",
      "--------------------------------------------------\n",
      "100\n",
      "100\n",
      "--------------------------------------------------\n",
      "67\n",
      "56\n",
      "--------------------------------------------------\n",
      "71\n",
      "71\n",
      "--------------------------------------------------\n",
      "55\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.QRatio(d1,d2))\n",
    "print(fuzz.QRatio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.partial_ratio(d1,d2))\n",
    "print(fuzz.partial_ratio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.WRatio(d1,d2))\n",
    "print(fuzz.WRatio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.partial_token_set_ratio(d1,d2))\n",
    "print(fuzz.partial_token_set_ratio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.partial_token_sort_ratio(d1,d2))\n",
    "print(fuzz.partial_token_sort_ratio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.token_set_ratio(d1,d2))\n",
    "print(fuzz.token_set_ratio(d2,d3))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print(fuzz.token_sort_ratio(d1,d2))\n",
    "print(fuzz.token_sort_ratio(d2,d3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "print(get_jaccard_sim(d1,d2))\n",
    "print(get_jaccard_sim(d2,d3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf + Distance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experience', 'feature', 'human', 'is', 'language', 'miracle', 'music', 'of', 'the', 'universal']\n"
     ]
    }
   ],
   "source": [
    "fitted = TfidfVectorizer().fit(d)\n",
    "transformed = fitted.transform(d)\n",
    "\n",
    "print(fitted.get_feature_names())\n",
    "\n",
    "a1 = transformed.toarray()[0]\n",
    "a2 = transformed.toarray()[1]\n",
    "a3 = transformed.toarray()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3549151836521609\n",
      "0.21373347085583771\n"
     ]
    }
   ],
   "source": [
    "print(1-spatial.distance.cosine(a1,a2))\n",
    "print(1-spatial.distance.cosine(a2,a3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf + SVD + Distance\n",
    "### [Variation](https://hub.packtpub.com/use-tensorflow-and-nlp-to-detect-duplicate-quora-questions-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d.copy()\n",
    "\n",
    "tfv_q1 = TfidfVectorizer()\n",
    "tfv_q2 = TfidfVectorizer() # Both must be same\n",
    "\n",
    "q1_tfidf = tfv_q1.fit_transform(d) # d is a collection of docs\n",
    "q2_tfidf = tfv_q2.fit_transform(d1) # d is a collection of other doc\n",
    "\n",
    "svd_q1 = TruncatedSVD(n_components=2)\n",
    "svd_q2 = TruncatedSVD(n_components=2)\n",
    "\n",
    "question1_vectors = svd_q1.fit_transform(q1_tfidf)\n",
    "question2_vectors2 = svd_q2.fit_transform(q2_tfidf)\n",
    "\n",
    "# Get Similarity of Cosine Dist Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings + Distance\n",
    "1. Get Vectors for each Word\n",
    "2. Average / Concat / Median all the Embedding of each word in Sentence / Document\n",
    "3. Calculate the Distance between 2\n",
    "\n",
    "**Distances**\n",
    "1. Word mover distance\n",
    "2. Normalized word mover distance\n",
    "3. Cosine distance between vectors of question1 and question2\n",
    "4. Manhattan distance between vectors of question1 and question2\n",
    "5. Jaccard similarity between vectors of question1 and question2\n",
    "6. Canberra distance between vectors of question1 and question2\n",
    "7. Euclidean distance between vectors of question1 and question2\n",
    "8. Minkowski distance between vectors of question1 and question2\n",
    "9. Braycurtis distance between vectors of question1 and question2\n",
    "10. The skew of the vector for question1\n",
    "11. The skew of the vector for question2\n",
    "12. The kurtosis of the vector for question1\n",
    "13. The kurtosis of the vector for question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings + TfIdf\n",
    "1. Get Vectors for each Word\n",
    "2. Multiply by the `idf` of each word\n",
    "3. Average / Concat / Median all the Embedding of each word in Sentence / Document\n",
    "4. Calculate the Distance between 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Superwised / AI / ML</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model\n",
    "1. Get Any / All of the features described in the methods above\n",
    "2. Pass the features to any Classification Model with `0/1` for `Not Similar / Similar` Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Model -> Embedding -> Distance\n",
    "\n",
    "1. Build a Model with 2 or 3 inputs. Any of `RNN / LSTM / GRU / Transformer / BERT`\n",
    "2. Train Model on `Contrastive / Triplet Loss`\n",
    "3. Extract Embedding for each Sentence\n",
    "4. Calculate distance between 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Unsuperwised</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf + Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANY Embeddings + Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA / LSA\n",
    "[1](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)\n",
    "[2](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/)\n",
    "[3]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
